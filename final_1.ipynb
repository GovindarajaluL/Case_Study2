{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ydwo1jsKfcH"
   },
   "source": [
    "# Bi-Directional Attention Flow (BiDAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "3MHQ7Mk2KfcQ",
    "outputId": "c9a3e0a3-519b-4946-eb22-05e59303f813"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "#import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "#import jsonlines\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Conv1D,Dense, Activation, Multiply, Add, Lambda,Conv2D ,Conv3D, \\\n",
    "MaxPooling1D,MaxPooling2D,Input, TimeDistributed, LSTM, Bidirectional,Flatten,Embedding,Dense,Dropout,Concatenate,AveragePooling1D\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "import tensorflow as tf\n",
    "from keras.activations import linear\n",
    "from keras.layers.advanced_activations import Softmax\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kas4mezKfdR"
   },
   "outputs": [],
   "source": [
    "''' The Highway is used to form a residual connection where T-transform_gate\n",
    "output=T * g(wy+b) + (1-T) * y\n",
    "where T->transform_gate sigma(wy+b)\n",
    "g(wy+b) -> transformed_data\n",
    "output shape(none,350)'''\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Activation, Multiply, Add, Lambda\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "class Highway(Layer):\n",
    "\n",
    "    activation = None\n",
    "    transform_gate_bias = None\n",
    "\n",
    "    def __init__(self, activation='relu', transform_gate_bias=-1, **kwargs):\n",
    "        self.activation = activation\n",
    "        self.transform_gate_bias = transform_gate_bias\n",
    "        super(Highway, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        dim = input_shape[-1]\n",
    "        transform_gate_bias_initializer = Constant(self.transform_gate_bias)\n",
    "        input_shape_dense_1 = input_shape[-1]\n",
    "        self.dense_1 = Dense(units=dim, bias_initializer=transform_gate_bias_initializer)\n",
    "        self.dense_1.build(input_shape)\n",
    "        self.dense_2 = Dense(units=dim)\n",
    "        self.dense_2.build(input_shape)\n",
    "        self.trainable_weights = self.dense_1.trainable_weights + self.dense_2.trainable_weights\n",
    "\n",
    "        super(Highway, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        dim = K.int_shape(x)[-1]\n",
    "        transform_gate = self.dense_1(x) #context-,350  ,que-,350 , transform_gate: ,350\n",
    "        transform_gate = Activation(\"sigmoid\")(transform_gate)#Z=T g(W y+b) + (1-T) y\n",
    "        carry_gate = Lambda(lambda x: 1.0 - x, output_shape=(dim,))(transform_gate)\n",
    "        transformed_data = self.dense_2(x)\n",
    "        transformed_data = Activation(self.activation)(transformed_data)\n",
    "        transformed_gated = Multiply()([transform_gate, transformed_data])\n",
    "        identity_gated = Multiply()([carry_gate, x])\n",
    "        value = Add()([transformed_gated, identity_gated])#shape(none,350)\n",
    "        return value\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['activation'] = self.activation\n",
    "        config['transform_gate_bias'] = self.transform_gate_bias\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLG_jxHpKfda"
   },
   "outputs": [],
   "source": [
    "''' calculating the Similarity between the query and the contex concatenation of both query , con\n",
    "tex vector and\n",
    "\n",
    "pairwise Similarity between query and contex\n",
    "input context_vectors (None, 250, 350),query_vectors (None, 35, 350)\n",
    "output(None,250,35)\n",
    "Z=W * [a:b:a*b]\n",
    "a->repeated_context_vectors\n",
    "b->repeated_query_vectors\n",
    "'''\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "tf.compat.v1.keras.backend.expand_dims\n",
    "\n",
    "class Similarity(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Similarity, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_similarity(self, repeated_context_vectors, repeated_query_vectors):\n",
    "        element_wise_multiply = repeated_context_vectors * repeated_query_vectors #element_wise_multiply (None, 250, 35, 350)\n",
    "        concatenated_tensor = K.concatenate(\n",
    "            [repeated_context_vectors, repeated_query_vectors, element_wise_multiply], axis=-1) #concatenated_tensor (None, 250, 35, 1050)\n",
    "        dot_product = K.squeeze(K.dot(concatenated_tensor, self.kernel), axis=-1)#dot_product (None, 250, 35)\n",
    "        return linear(dot_product + self.bias)#(None,250,35)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        word_vector_dim = input_shape[0][-1]\n",
    "        weight_vector_dim = word_vector_dim * 3\n",
    "        self.kernel = self.add_weight(name='similarity_weight',\n",
    "                                      shape=(weight_vector_dim, 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='similarity_bias',\n",
    "                                    shape=(),\n",
    "                                    initializer='ones',\n",
    "                                    trainable=True)\n",
    "        super(Similarity, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context_vectors, query_vectors = inputs  #context_vectors (None, 250, 350) ,query_vectors(None, 35, 350)\n",
    "        num_context_words = K.shape(context_vectors)[1] #num_context_words(250)\n",
    "        num_query_words = K.shape(query_vectors)[1] #num_query_words(35)\n",
    "        context_dim_repeat = K.concatenate([[1, 1], [num_query_words], [1]], 0)#[1,1,35,1]\n",
    "        query_dim_repeat = K.concatenate([[1], [num_context_words], [1, 1]], 0)#[1,250,1,1]\n",
    "         #(None, 250, 1, 350) (None, 1, 35, 350) \n",
    "        repeated_context_vectors = K.tile(tf.compat.v1.keras.backend.expand_dims(context_vectors, axis=2), context_dim_repeat)\n",
    "        repeated_query_vectors = K.tile(tf.compat.v1.keras.backend.expand_dims(query_vectors, axis=1), query_dim_repeat)\n",
    "        #repeated_context_vectors (None, 250, 35, 350),repeated_query_vectors (None, 250, 35, 350)\n",
    "        similarity_matrix = self.compute_similarity(repeated_context_vectors, repeated_query_vectors)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size = input_shape[0][0]\n",
    "        num_context_words = input_shape[0][1]\n",
    "        num_query_words = input_shape[1][1]\n",
    "        return (batch_size, num_context_words, num_query_words)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4nXYU-NKfdi"
   },
   "outputs": [],
   "source": [
    "'''Context-to-Query Attention taking the row-wise softmax of Similarity and multiply with question\n",
    "vector\n",
    "input similarity_matrix(None, 250, 35),encoded_question(None, 35, 350)\n",
    "output (None, 250, 350)'''\n",
    "\n",
    "\n",
    "class C2QAttention(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(C2QAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(C2QAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        similarity_matrix, encoded_question = inputs #similarity_matrix(None, 250, 35),encoded_question(None, 35, 350)\n",
    "        context_to_query_attention = Softmax(axis=-1)(similarity_matrix)#context_to_query_attention (None, 250, 35)\n",
    "        encoded_question = K.expand_dims(encoded_question, axis=1)#encoded_question(None, 1, 35, 350)\n",
    "        #(None, 250, 35, 1)\n",
    "        return K.sum(K.expand_dims(context_to_query_attention, axis=-1) * encoded_question, -2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        similarity_matrix_shape, encoded_question_shape = input_shape\n",
    "        return similarity_matrix_shape[:-1] + encoded_question_shape[-1:]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzrQBKheKfdr"
   },
   "outputs": [],
   "source": [
    "'''Query-to-Context (Q2C) Attention we taking the max in row-wise of similarity_matrix and applyin\n",
    "g softmax at last\n",
    "multiply with contex vector\n",
    "input similarity_matrix(None, 250, 35),encoded_context(None, 250, 350)\n",
    "output (None, None, 350)\n",
    "'''\n",
    "\n",
    "class Q2CAttention(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Q2CAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Q2CAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        similarity_matrix, encoded_context = inputs #similarity_matrix(None, 250, 35),encoded_context(None, 250, 350)\n",
    "        max_similarity = K.max(similarity_matrix, axis=-1) #max_similarity(None, 250)\n",
    "        # by default, axis = -1 in Softmax\n",
    "        context_to_query_attention = Softmax()(max_similarity)#context_to_query_attention(None, 250)\n",
    "        weighted_sum = K.sum(K.expand_dims(context_to_query_attention, axis=-1) * encoded_context, -2)#weighted_sum(None, 350)\n",
    "        expanded_weighted_sum = K.expand_dims(weighted_sum, 1)#expanded_weighted_sum (None, 1, 350)\n",
    "        num_of_repeatations = K.shape(encoded_context)[1]\n",
    "        return K.tile(expanded_weighted_sum, [1, num_of_repeatations, 1])#(None, 250, 350)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        similarity_matrix_shape, encoded_context_shape = input_shape\n",
    "        return similarity_matrix_shape[:-1] + encoded_context_shape[-1:]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fWVxJUxKfdz"
   },
   "outputs": [],
   "source": [
    "'''MergedContext multiply the contex vector with Context-to-Query and Query-to-Context vectors at\n",
    "last concatenated\n",
    "contex,context_to_query,multiply1,multiply2\n",
    "input encoded_context(None, 250, 350),context_to_query_attention(None, 250,\n",
    "350),query_to_context_attention(None, None, 350)\n",
    "output (None, 250, 1400)\n",
    "'''\n",
    "\n",
    "\n",
    "class MergedContext(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MergedContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(MergedContext, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded_context, context_to_query_attention, query_to_context_attention = inputs\n",
    "        #encoded_context(None, 250, 350),context_to_query_attention(None, 250,350),query_to_context_attention(None, 250, 350)\n",
    "        element_wise_multiply1 = encoded_context * context_to_query_attention #element_wise_multiply1 (None, 250, 350)\n",
    "        element_wise_multiply2 = encoded_context * query_to_context_attention #element_wise_multiply2 (None, 250, 350)\n",
    "        concatenated_tensor = K.concatenate(\n",
    "            [encoded_context, context_to_query_attention, element_wise_multiply1, element_wise_multiply2], axis=-1)\n",
    "        return concatenated_tensor #(None, 250, 1400)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        encoded_context_shape, _, _ = input_shape\n",
    "        return encoded_context_shape[:-1] + (encoded_context_shape[-1] * 4, )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErcobPNdKfeM"
   },
   "outputs": [],
   "source": [
    "'''CombineOutputs use to stack the inputs'''\n",
    "class CombineOutputs(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CombineOutputs, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(CombineOutputs, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        span_begin_probabilities, span_end_probabilities,span_begin_probabilities_1,span_end_probabilities_1,span_end_probabilities_2 = inputs\n",
    "        return K.stack([span_begin_probabilities, span_end_probabilities,span_begin_probabilities_1,span_end_probabilities_1,span_end_probabilities_2 ], axis = 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        number_of_tensors = len(input_shape)\n",
    "        return input_shape[0][0:1] + (number_of_tensors, ) + input_shape[0][1:]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIJ7QdmaKfej"
   },
   "outputs": [],
   "source": [
    "'''MagnitudeVectors use to Loading the glove vector '''\n",
    "import os\n",
    "from pymagnitude import Magnitude, MagnitudeUtils\n",
    "\n",
    "\n",
    "class MagnitudeVectors():\n",
    "\n",
    "    def __init__(self, emdim):\n",
    "\n",
    "        base_dir = \"challenge\"\n",
    "\n",
    "        self.fasttext_dim = 300\n",
    "        self.glove_dim = emdim - 300\n",
    "\n",
    "        assert self.glove_dim in [50, 100, 200,\n",
    "                                  300], \"Embedding dimension must be one of the following: 350, 400, 500, 600\"\n",
    "\n",
    "       # print(\"Will download magnitude files from the server if they aren't avaialble locally.. So, grab a cup of coffee while the downloading is under progress..\")\n",
    "        glove = Magnitude(MagnitudeUtils.download_model('glove/medium/glove.6B.{}d'.format(self.glove_dim),\n",
    "                                                        download_dir=os.path.join(base_dir, 'magnitude')), case_insensitive=True)\n",
    "        fasttext = Magnitude(MagnitudeUtils.download_model('fasttext/medium/wiki-news-300d-1M-subword',\n",
    "                                                           download_dir=os.path.join(base_dir, 'magnitude')), case_insensitive=True)\n",
    "        self.vectors = Magnitude(glove, fasttext)\n",
    "\n",
    "    def load_vectors(self):\n",
    "        return self.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VldyRlqxKfey"
   },
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "class ModelMGPU(Model):\n",
    "    def __init__(self, ser_model, gpus=None):\n",
    "        pmodel = multi_gpu_model(ser_model, gpus)\n",
    "        self.__dict__.update(pmodel.__dict__)\n",
    "        self._smodel = ser_model\n",
    "\n",
    "    def __getattribute__(self, attrname):\n",
    "        '''Override load and save methods to be used from the serial-model. The\n",
    "        serial-model holds references to the weights in the multi-gpu model.\n",
    "        '''\n",
    "        # return Model.__getattribute__(self, attrname)\n",
    "        if 'load' in attrname or 'save' in attrname:\n",
    "            return getattr(self._smodel, attrname)\n",
    "\n",
    "        return super(ModelMGPU, self).__getattribute__(attrname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adadelta,Adam\n",
    "from scipy.sparse import hstack\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "class BidirectionalAttentionFlow():\n",
    "\n",
    "    def __init__(self, emdim, max_passage_length=None, max_query_length=None, num_highway_layers=2, num_decoders=1,\n",
    "                 encoder_dropout=0, decoder_dropout=0):\n",
    "        self.emdim = emdim\n",
    "        self.max_passage_length =250\n",
    "        self.max_query_length = 35\n",
    "\n",
    "        passage_input = Input(shape=(self.max_passage_length, emdim), dtype='float32', name=\"passage_input\")\n",
    "        question_input = Input(shape=(self.max_query_length, emdim), dtype='float32', name=\"question_input\")\n",
    "\n",
    "        choice_input_1 = Input(shape=(8, emdim), dtype='float32', name=\"choice_input_1\")\n",
    "        choice_input_2 = Input(shape=(8, emdim), dtype='float32', name=\"choice_input_2\")\n",
    "        choice_input_3 = Input(shape=(8, emdim), dtype='float32', name=\"choice_input_3\")\n",
    "        choice_input_4 = Input(shape=(8, emdim), dtype='float32', name=\"choice_input_4\")\n",
    "        choice_input_5 = Input(shape=(8, emdim), dtype='float32', name=\"choice_input_5\")\n",
    "\n",
    "        question_embedding = question_input #35,350\n",
    "        passage_embedding = passage_input #250,350\n",
    "        #choice_embedding = choice_input\n",
    "        for i in range(num_highway_layers):\n",
    "            highway_layer = Highway(name='highway_{}'.format(i))\n",
    "            question_layer = TimeDistributed(highway_layer, name=highway_layer.name + \"_qtd\")\n",
    "            question_embedding = question_layer(question_embedding)\n",
    "            passage_layer = TimeDistributed(highway_layer, name=highway_layer.name + \"_ptd\")\n",
    "            passage_embedding = passage_layer(passage_embedding)\n",
    "           \n",
    "\n",
    "        encoder_layer = Bidirectional(LSTM(175, recurrent_dropout=encoder_dropout,\n",
    "                                           return_sequences=True), name='bidirectional_encoder')\n",
    "        encoded_question = encoder_layer(question_embedding)\n",
    "        encoded_passage = encoder_layer(passage_embedding)\n",
    "        \n",
    "        #encoded_passage (None, 250, 350),encoded_question (None, 35, 350)\n",
    "        similarity_matrix = Similarity(name='similarity_layer')([encoded_passage, encoded_question])#(None,250,35)\n",
    "        #i,j represents the similarity between the ith word in context and jth word in the query\n",
    "\n",
    "        context_to_query_attention = C2QAttention(name='context_to_query_attention')([\n",
    "            similarity_matrix, encoded_question])#(None, 250, 350)\n",
    "        query_to_context_attention = Q2CAttention(name='query_to_context_attention')([\n",
    "            similarity_matrix, encoded_passage])#(None, 250, 350)\n",
    "      # 100, (i,j) ==?\n",
    "\n",
    "        merged_context = MergedContext(name='merged_context')(\n",
    "            [encoded_passage, context_to_query_attention, query_to_context_attention])#(None, 250, 1400)\n",
    "        \n",
    "        modeled_passage = merged_context\n",
    "        \n",
    "        for i in range(num_decoders):\n",
    "            hidden_layer = Bidirectional(LSTM(175, recurrent_dropout=decoder_dropout,\n",
    "                                              return_sequences=True), name='bidirectional_decoder_{}'.format(i))\n",
    "            modeled_passage = hidden_layer(modeled_passage)#(None, 250, 350)\n",
    "\n",
    "        #choice_input None,8,350\n",
    "        similarity_matrix_choice_1 = Similarity(name='similarity_layer_choice_1')([ choice_input_1,modeled_passage])\n",
    "        similarity_matrix_choice_2 = Similarity(name='similarity_layer_choice_2')([ choice_input_2,modeled_passage])\n",
    "        similarity_matrix_choice_3 = Similarity(name='similarity_layer_choice_3')([ choice_input_3,modeled_passage])\n",
    "        similarity_matrix_choice_4 = Similarity(name='similarity_layer_choice_4')([ choice_input_4,modeled_passage])\n",
    "        similarity_matrix_choice_5 = Similarity(name='similarity_layer_choice_5')([ choice_input_5,modeled_passage])\n",
    "\n",
    "        #similarity_matrix_choice (None, 8, 250)\n",
    "        merged_context = CombineOutputs(name='combine_outputs')(\n",
    "            [similarity_matrix_choice_1, similarity_matrix_choice_2, similarity_matrix_choice_3,similarity_matrix_choice_4,similarity_matrix_choice_5])\n",
    "        \n",
    "        modeled_passage = merged_context#(None, 5, 8, 250)\n",
    "        \n",
    "        den=Dense(150, activation='relu')( modeled_passage) #(None, 5, 8, 150) \n",
    "        dropout_layer3=Dropout(rate=0.5)(den)\n",
    "        den=Dense(70, activation='relu')( dropout_layer3)#(None, 5, 8, 70) \n",
    "        dropout_layer3=Dropout(rate=0.5)(den)\n",
    "        \n",
    "        \n",
    "        flat_out=Flatten()(dropout_layer3)#(None, 2800)\n",
    "        output=Dense(5, activation='softmax')( flat_out )#(None, 5)\n",
    "       \n",
    "        \n",
    "        model = Model([passage_input, question_input,choice_input_1,choice_input_2,choice_input_3,choice_input_4,choice_input_5],[output])\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "\n",
    "        try:\n",
    "            model = ModelMGPU(model)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def load_bidaf(self, path):\n",
    "        custom_objects = {\n",
    "            'Highway': Highway,\n",
    "            'Similarity': Similarity,\n",
    "            'C2QAttention': C2QAttention,\n",
    "            'Q2CAttention': Q2CAttention,\n",
    "            'MergedContext': MergedContext,\n",
    "            'CombineOutputs': CombineOutputs\n",
    "        }\n",
    "\n",
    "        self.model = load_model(path, custom_objects=custom_objects)\n",
    "\n",
    "    def train_model(self, train_generator, steps_per_epoch=None, epochs=1, validation_generator=None,\n",
    "                    validation_steps=None,  use_multiprocessing=False, shuffle=True, initial_epoch=0,\n",
    "                    save_history=False, save_model_per_epoch=True):\n",
    "\n",
    "        saved_items_dir = \"/content/drive/My Drive/case_study1/saved_items\"\n",
    "        if not os.path.exists(saved_items_dir):\n",
    "            os.makedirs(saved_items_dir)\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        if save_history:\n",
    "            history_file = os.path.join(saved_items_dir, 'history')\n",
    "            csv_logger = CSVLogger(history_file, append=True)\n",
    "            callbacks.append(csv_logger)\n",
    "\n",
    "        if save_model_per_epoch:\n",
    "            save_model_file = os.path.join(saved_items_dir, 'bidaf_{epoch:02d}.h5')\n",
    "            log_dir=\"saved_items/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            checkpoint_path = \"saved_items/weights-improvement.hdf5\" \n",
    "            checkpoint = ModelCheckpoint(checkpoint_path, verbose=1)\n",
    "            #callbacks_list = [checkpoint,tensorboard_callback]\n",
    "            callbacks_list = [checkpoint]\n",
    "\n",
    "        history = self.model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "                                           callbacks=callbacks_list, validation_data=validation_generator,\n",
    "                                           validation_steps=validation_steps, \n",
    "                                           use_multiprocessing=use_multiprocessing, shuffle=shuffle,\n",
    "                                           initial_epoch=initial_epoch,class_weight='auto')\n",
    "        if save_model_per_epoch:\n",
    "            self.model.save(os.path.join(saved_items_dir, 'bidaf.h5'))\n",
    "\n",
    "        return history, self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bSN5kNGUKffC",
    "outputId": "69d212b3-c480-47e0-fcbf-47e36dcaee7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "passage_input (InputLayer)      (None, 250, 350)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     (None, 35, 350)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "highway_0_ptd (TimeDistributed) (None, 250, 350)     245700      passage_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "highway_0_qtd (TimeDistributed) (None, 35, 350)      245700      question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "highway_1_ptd (TimeDistributed) (None, 250, 350)     245700      highway_0_ptd[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "highway_1_qtd (TimeDistributed) (None, 35, 350)      245700      highway_0_qtd[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_encoder (Bidirect multiple             736400      highway_1_qtd[0][0]              \n",
      "                                                                 highway_1_ptd[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer (Similarity)   (None, 250, 35)      1051        bidirectional_encoder[1][0]      \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "context_to_query_attention (C2Q (None, 250, 350)     0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "query_to_context_attention (Q2C (None, 250, 350)     0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "merged_context (MergedContext)  (None, 250, 1400)    0           bidirectional_encoder[1][0]      \n",
      "                                                                 context_to_query_attention[0][0] \n",
      "                                                                 query_to_context_attention[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_decoder_0 (Bidire (None, 250, 350)     2206400     merged_context[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "choice_input_1 (InputLayer)     (None, 8, 350)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_decoder_1 (Bidire (None, 250, 350)     736400      bidirectional_decoder_0[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "choice_input_2 (InputLayer)     (None, 8, 350)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "choice_input_3 (InputLayer)     (None, 8, 350)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "choice_input_4 (InputLayer)     (None, 8, 350)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "choice_input_5 (InputLayer)     (None, 8, 350)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer_choice_1 (Simi (None, 8, 250)       1051        choice_input_1[0][0]             \n",
      "                                                                 bidirectional_decoder_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer_choice_2 (Simi (None, 8, 250)       1051        choice_input_2[0][0]             \n",
      "                                                                 bidirectional_decoder_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer_choice_3 (Simi (None, 8, 250)       1051        choice_input_3[0][0]             \n",
      "                                                                 bidirectional_decoder_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer_choice_4 (Simi (None, 8, 250)       1051        choice_input_4[0][0]             \n",
      "                                                                 bidirectional_decoder_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer_choice_5 (Simi (None, 8, 250)       1051        choice_input_5[0][0]             \n",
      "                                                                 bidirectional_decoder_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "combine_outputs (CombineOutputs (None, 5, 8, 250)    0           similarity_layer_choice_1[0][0]  \n",
      "                                                                 similarity_layer_choice_2[0][0]  \n",
      "                                                                 similarity_layer_choice_3[0][0]  \n",
      "                                                                 similarity_layer_choice_4[0][0]  \n",
      "                                                                 similarity_layer_choice_5[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 5, 8, 150)    37650       combine_outputs[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 5, 8, 150)    0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5, 8, 70)     10570       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 5, 8, 70)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2800)         0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 5)            14005       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,239,131\n",
      "Trainable params: 4,239,131\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "bidaf_model = BidirectionalAttentionFlow(emdim=350, max_passage_length=None,\n",
    "                                             max_query_length=None,\n",
    "                                             num_highway_layers=2, num_decoders=2,\n",
    "                                             encoder_dropout=0.5, decoder_dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ks4qSUaKffQ"
   },
   "outputs": [],
   "source": [
    "bidaf_model.model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidaf_model.load_bidaf('saved_items/weights-improvement (5).hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zqtiyvFKffa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data_generators(batch_size, emdim, squad_version=1.1, max_passage_length=None, max_query_length=None,\n",
    "                         shuffle=False):\n",
    "    train_generator = BatchGenerator('train', batch_size, emdim, squad_version, max_passage_length, max_query_length,\n",
    "                                     shuffle)\n",
    "    validation_generator = BatchGenerator('dev', batch_size, emdim, squad_version, max_passage_length, max_query_length,\n",
    "                                          shuffle)\n",
    "    return train_generator, validation_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-27YK_zCsj2"
   },
   "outputs": [],
   "source": [
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qb8mNqcqKffn"
   },
   "outputs": [],
   "source": [
    "'''BatchGenerator use to generate the batch of inputes contex with 250 words and query with 35 words and choice with 8 words\n",
    "i also used all the Easy and Challenge both combained data for train and validate '''\n",
    "\n",
    "from keras.utils import Sequence\n",
    "\n",
    "\n",
    "\n",
    "class BatchGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    vectors = None\n",
    "\n",
    "    def __init__(self, gen_type, batch_size, emdim, squad_version, max_passage_length, max_query_length, shuffle):\n",
    "        'Initialization'\n",
    "\n",
    "        base_dir = \"storage/\"\n",
    "\n",
    "        self.vectors = MagnitudeVectors(emdim).load_vectors()\n",
    "        self.squad_version = squad_version\n",
    "\n",
    "        self.max_passage_length = max_passage_length\n",
    "        self.max_query_length = max_query_length\n",
    "\n",
    "        self.context_file = os.path.join('all_data', gen_type + '-v{}.cha_context'.format(squad_version))\n",
    "        self.question_file = os.path.join('all_data', gen_type + '-v{}.cha_question'.format(squad_version))\n",
    "        self.span_file_ans = os.path.join('all_data', gen_type + '-v{}.cha_choices'.format(squad_version))\n",
    "        self.span_file = os.path.join('all_data', gen_type + '-v{}.cha_answer'.format(squad_version))\n",
    "        \n",
    "        self.gen_type = gen_type\n",
    "        self.batch_size = batch_size\n",
    "        i = 0\n",
    "        with open(self.span_file, 'r', encoding='utf-8') as f:\n",
    "\n",
    "            for i, _ in enumerate(f):\n",
    "                pass\n",
    "        self.num_of_batches = (i + 1) // self.batch_size\n",
    "        self.indices = np.arange(i + 1)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.num_of_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        start_index = (index * self.batch_size) + 1\n",
    "        end_index = ((index + 1) * self.batch_size) + 1\n",
    "\n",
    "        inds = self.indices[start_index:end_index]\n",
    "\n",
    "        contexts = []\n",
    "        with open(self.context_file, 'r', encoding='utf-8') as cf:\n",
    "            for i, line in enumerate(cf, start=1):\n",
    "                line = line[:-1]\n",
    "                if i in inds:\n",
    "                    word_no=[]\n",
    "                    count=0\n",
    "                    for word in line.split(' '):\n",
    "                        if word.lower() not in stopwords and count<250 and word.strip() !='':\n",
    "                            count=count+1\n",
    "                            word_no.append(word.lower().strip())\n",
    "                    if count<250:\n",
    "                        for i in range(250 - count):\n",
    "                            word_no.append(0)    \n",
    "                    contexts.append(word_no)\n",
    "\n",
    "        questions = []\n",
    "        with open(self.question_file, 'r', encoding='utf-8') as qf:\n",
    "            for i, line in enumerate(qf, start=1):\n",
    "                line = line[:-1]\n",
    "                if i in inds:\n",
    "                    word_no=[]\n",
    "                    count=0\n",
    "                    for word in line.split(' '):\n",
    "                        #for i in word.split(','):\n",
    "                        if word.lower().strip() not in stopwords and count<35 and word.strip() !='':\n",
    "                            count=count+1\n",
    "                            word_no.append(word.lower().strip())\n",
    "                    if count<35:\n",
    "                        for i in range(35 - count):\n",
    "                            word_no.append(0)    \n",
    "                    questions.append(word_no)\n",
    "\n",
    "        answer_spans = []\n",
    "        with open(self.span_file, 'r', encoding='utf-8') as sf:\n",
    "            for i, line in enumerate(sf, start=1):\n",
    "                line = line[:-1]\n",
    "                if i in inds:\n",
    "                        line=line.strip()\n",
    "                        if line=='A' or line=='1':\n",
    "                            answer_spans.append([1,0,0,0,0])\n",
    "                        if line=='B' or line=='2':\n",
    "                            answer_spans.append([0,1,0,0,0])\n",
    "                        if line=='C' or line=='3':\n",
    "                            answer_spans.append([0,0,1,0,0])\n",
    "                        if line=='D' or line=='4':\n",
    "                            answer_spans.append([0,0,0,1,0])\n",
    "                        if line=='E':\n",
    "                            answer_spans.append([0,0,0,0,1])\n",
    "\n",
    "\n",
    "\n",
    "        answer_contexts = []\n",
    "        answer_choice_1=[]\n",
    "        answer_choice_2=[]\n",
    "        answer_choice_3=[]\n",
    "        answer_choice_4=[]\n",
    "        answer_choice_5=[]\n",
    "        \n",
    "        with open(self.span_file_ans, 'r', encoding='utf-8') as sf:\n",
    "            for i, line in enumerate(sf, start=1):\n",
    "                line = line[:-1]\n",
    "                if i in inds:\n",
    "                    word_no2=[]\n",
    "                    last_count=0\n",
    "                    for word in line.split(','):\n",
    "                        count=0\n",
    "                        word_no=[]\n",
    "                        for i in word.split(' '):\n",
    "                              if count<8 and i.strip() !='':\n",
    "                                    count=count+1\n",
    "                                    word_no.append(i.lower().strip())\n",
    "                    \n",
    "                        if count<8:\n",
    "                              for i in range(8 - count):\n",
    "                                      word_no.append(0)  \n",
    "                        word_no2.append(word_no)\n",
    "                        last_count=last_count+1\n",
    "                    if  last_count<5:\n",
    "                        last_count=last_count+1\n",
    "                        last_word_no=[]\n",
    "                        for i in range(8):\n",
    "                              last_word_no.append(0)\n",
    "                        word_no2.append(last_word_no)\n",
    "\n",
    "                    if  last_count<5:\n",
    "                        last_count=last_count+1\n",
    "                        last_word_no=[]\n",
    "                        for i in range(8):\n",
    "                              last_word_no.append(0)\n",
    "                        word_no2.append(last_word_no)\n",
    "                        \n",
    "                    answer_choice_1.append(word_no2[0])\n",
    "                    answer_choice_2.append(word_no2[1])\n",
    "                    answer_choice_3.append(word_no2[2])\n",
    "                    answer_choice_4.append(word_no2[3])\n",
    "                    answer_choice_5.append(word_no2[4])\n",
    "\n",
    "                    #answer_contexts.append(self.vectors.query(word_no2, pad_to_length=350))\n",
    "                   \n",
    "\n",
    "        context_batch = self.vectors.query(contexts, pad_to_length=self.max_passage_length)\n",
    "        question_batch = self.vectors.query(questions, pad_to_length=self.max_query_length)\n",
    "        #answer_contexts_batch = np.array(answer_contexts)\n",
    "        \n",
    "        answer_contexts_batch_1 = self.vectors.query(answer_choice_1, pad_to_length=self.max_query_length)\n",
    "        answer_contexts_batch_2 = self.vectors.query(answer_choice_2, pad_to_length=self.max_query_length)\n",
    "        answer_contexts_batch_3 = self.vectors.query(answer_choice_3, pad_to_length=self.max_query_length)\n",
    "        answer_contexts_batch_4 = self.vectors.query(answer_choice_4, pad_to_length=self.max_query_length)\n",
    "        answer_contexts_batch_5 = self.vectors.query(answer_choice_5, pad_to_length=self.max_query_length)\n",
    "        \n",
    "        return [context_batch, question_batch,answer_contexts_batch_1,answer_contexts_batch_2,answer_contexts_batch_3,answer_contexts_batch_4,answer_contexts_batch_5], [answer_spans]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generate the batch and train the model with batch 15 and epoch 20'''\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import warnings \n",
    "from time import time\n",
    "\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_generator, validation_generator = load_data_generators(batch_size=15, emdim=350,\n",
    "                                                                     squad_version=1.1,\n",
    "                                                                     max_passage_length=None,\n",
    "                                                                     max_query_length=None,\n",
    "                                                                     shuffle=False)\n",
    "\n",
    "bidaf_model.train_model(train_generator, steps_per_epoch=None, epochs=20,\n",
    "                                validation_generator=validation_generator, validation_steps=None, use_multiprocessing=False,\n",
    "                                shuffle=False, save_history=False,\n",
    "                                save_model_per_epoch=True)\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvAblg_R2arA"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data_generators_test(batch_size, emdim, squad_version=1.1, max_passage_length=None, max_query_length=None,\n",
    "                         shuffle=False):\n",
    "    test_generator = BatchGenerator('test', batch_size, emdim, squad_version, max_passage_length, max_query_length,\n",
    "                                     shuffle)\n",
    "    \n",
    "    return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "4xidHgPpKff2",
    "outputId": "4a475d1c-2c9b-4dba-dc2c-4d337d3c905a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download magnitude files from the server if they aren't avaialble locally.. So, grab a cup of coffee while the downloading is under progress..\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "from time import time\n",
    "\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''I have used only the Challenge data for testing '''\n",
    "\n",
    "test_generator = load_data_generators_test(batch_size=15, emdim=350,\n",
    "                                                                     squad_version=1.1,\n",
    "                                                                     max_passage_length=None,\n",
    "                                                                     max_query_length=None,\n",
    "                                                                     shuffle=False)\n",
    "\n",
    "loss,acc = bidaf_model.model.evaluate(test_generator, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_out=bidaf_model.model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "-4IPP_gYAjdR",
    "outputId": "6ce4af15-150b-4598-ac25-f874c7384012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.2777777910232544  Test Loss:  1.4846593141555786\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: \",acc,\" Test Loss: \",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Leader board The BiDAF model has accuracy of 26.54 %  here i have improved the model as 27.77 %\n",
    "\n",
    "https://leaderboard.allenai.org/arc/submissions/public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) While sending the context words more than 300 the model is not improving the loss \n",
    "\n",
    "2) Mode Over fit while batch size is 5,10\n",
    "\n",
    "3) if the padding is more in contex,query the model not improving the loss\n",
    "\n",
    "4) sending the choices at single string and calculating the similarity also go to over fitting of model \n",
    "\n",
    "5) sending the choices with question also not improving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Case_Study_BiDaf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
